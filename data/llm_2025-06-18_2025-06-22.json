[
  {
    "title": "PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning",
    "summary": "With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.",
    "link": "http://arxiv.org/abs/2506.15683v1",
    "published": "2025-06-18T17:59:58+00:00",
    "summary_zh": "随着大语言模型（LLMs）的普及，虚假信息生成和学术不端等不良社会问题日益严重，这使得大语言模型生成文本检测工作的重要性达到了前所未有的高度。尽管现有方法已取得显著进展，但私人微调大语言模型生成的文本所带来的新挑战仍未得到充分研究。用户通过使用私有语料对开源模型进行微调，即可轻易拥有私人定制的大语言模型，这导致现有检测器在实际应用中性能大幅下降。为解决这一问题，我们提出PhantomHunter——一种专门用于检测未知私人微调大语言模型生成文本的检测器。其家族感知学习框架能够捕捉基础模型及其衍生版本共享的家族级特征，而非记忆个体特性。在LLaMA、Gemma和Mistral家族数据上的实验表明，该检测器性能超越7个基线模型和3项工业服务，F1分数超过96%。"
  },
  {
    "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models",
    "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
    "link": "http://arxiv.org/abs/2506.15681v1",
    "published": "2025-06-18T17:59:49+00:00",
    "summary_zh": "视觉-语言模型（VLMs）的最新进展通过利用大语言模型（LLMs），已实现了与GPT-4V等闭源系统相当的性能表现。然而，由于这些模型存在巨大的计算需求，在实际场景中（尤其是资源受限设备）部署仍面临挑战。这促使学界开始研究将大型VLMs的知识蒸馏到更小、更高效的模型中。这一过程中存在的关键挑战源于VLM架构的多样性——这些架构基于不同的LLMs构建，并采用存在词汇量、分词方式及标记索引顺序差异的多样化标记类型。为突破特定VLM类型的限制，我们提出了一种新型通用蒸馏框架\"重校准后生成\"（GenRecal）。该框架通过引入重校准器来对齐和适配异构VLMs间的特征表示，从而实现跨不同类型VLMs的有效知识迁移。通过在多个具有挑战性的基准测试上进行大量实验，我们证明GenRecal能显著提升基线模型性能，最终超越大规模开源与闭源VLMs。"
  },
  {
    "title": "CC-LEARN: Cohort-based Consistency Learning",
    "summary": "Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.",
    "link": "http://arxiv.org/abs/2506.15662v1",
    "published": "2025-06-18T17:41:28+00:00",
    "summary_zh": "大型语言模型在众多任务中表现优异，但在保持一致且稳健的推理能力方面仍存在困难。我们提出基于队列的一致性学习（CC-Learn）——一种强化学习框架，通过利用共享程序化抽象生成的相似问题队列进行训练，从而提升大语言模型的推理可靠性。为确保队列层面的推理一致性，我们定义了一个复合目标函数，该函数综合了队列准确率、针对有效问题分解的检索奖励机制，以及对琐碎或无效查询的拒绝惩罚项。与监督微调不同，强化学习可直接优化这一奖励函数。通过优化该奖励机制，模型能够对所有队列成员采用统一的推理模式。在ARC-Challenge和StrategyQA等具有挑战性的推理基准测试中，实验结果表明CC-Learn在准确率和推理稳定性方面均优于预训练模型和监督微调基线。这些成果证明，基于队列的强化学习能有效增强大语言模型的推理一致性。"
  },
  {
    "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection",
    "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements.",
    "link": "http://arxiv.org/abs/2506.15656v1",
    "published": "2025-06-18T17:33:18+00:00",
    "summary_zh": "钓鱼网站持续构成重大网络安全威胁，其往往通过欺骗性结构、品牌仿冒及社会工程学手段规避检测。尽管大语言模型（LLM）的最新进展已通过上下文理解提升了钓鱼检测能力，但现有方法多依赖单一智能体分类机制，存在幻觉风险且缺乏可解释性与鲁棒性。针对这些局限，我们提出PhishDebate——一种基于模块化多智能体LLM的钓鱼网站检测辩论框架。该框架通过协调员与最终裁决者的调度，部署四个专业智能体分别独立分析网页的URL结构、HTML构成、语义内容及品牌仿冒等文本特征。通过结构化辩论与发散思维，该框架能输出更精准且可解释的判定结果。在商业LLM上的大量评估表明，PhishDebate在真实钓鱼数据集上实现了98.2%的召回率与98.2%的真正例率（TPR），性能显著优于单一智能体及思维链（CoT）基线方法。其模块化设计支持智能体级别的配置调整，可适应不同资源条件与应用需求。"
  },
  {
    "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses",
    "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.",
    "link": "http://arxiv.org/abs/2506.15648v1",
    "published": "2025-06-18T17:18:23+00:00",
    "summary_zh": "尽管Rust默认保证内存安全，但也允许使用可能因误用而引发内存安全漏洞的unsafe代码。遗憾的是，现有Rust内存错误检测工具普遍存在检测能力有限、对Rust特有类型处理不足或过度依赖人工干预等问题。\n\n为解决这些局限，我们提出deepSURF工具——该工具通过整合静态分析与大型语言模型（LLM）引导的模糊测试用例生成技术，可有效识别Rust库（特别是unsafe代码部分）中的内存安全漏洞。deepSURF创新性地采用自定义类型替换泛型并为所需trait生成定制化实现的方法，使模糊测试器能够模拟被测库中的用户定义行为。此外，deepSURF运用LLM动态增强模糊测试用例，促进复杂API交互的探索，显著提升内存安全漏洞暴露概率。我们在27个真实Rust包上评估表明，deepSURF成功复现了20个已知内存安全漏洞，并发现6个既往未知漏洞，较现有最先进工具展现出明显优势。"
  },
  {
    "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability",
    "summary": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.",
    "link": "http://arxiv.org/abs/2506.15629v1",
    "published": "2025-06-18T17:00:54+00:00",
    "summary_zh": "在CommonGen等生成式常识推理任务中，生成式大语言模型（LLMs）会组合包含所有给定概念的句子。然而当聚焦指令遵循能力时，若提示词指定了概念顺序，LLMs必须生成符合该顺序的句子。为此我们提出Ordered CommonGen基准测试，用于评估LLMs的组合泛化与指令遵循能力。该基准通过测量有序覆盖率来评估概念是否按指定顺序生成，从而实现对两种能力的同步评价。我们对36个LLMs进行综合分析发现：虽然LLMs普遍能理解指令意图，但对特定概念顺序模式的偏好常导致输出多样性不足——即便改变概念顺序也会产生雷同结果。值得注意的是，即便最遵循指令的LLM其有序覆盖率也仅约75%，这表明在指令遵循和组合泛化能力两方面均需改进。"
  },
  {
    "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games",
    "summary": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.",
    "link": "http://arxiv.org/abs/2506.15624v1",
    "published": "2025-06-18T16:53:38+00:00",
    "summary_zh": "大型语言模型（LLMs）在动态环境中展现出作为决策者的潜力，但其无状态特性要求构建自然语言形式的历史表征。我们提出一个统一框架，用于系统化构建自然语言\"状态\"表征，以在重复多智能体博弈中引导LLM智能体。既有研究对LLM智能体博弈的历史编码多采用临时性方案，这不仅模糊了状态表征对智能体行为的影响机制，也限制了不同研究间的可比性。我们的框架通过三个维度刻画状态表征方法：动作信息量（即状态表征对已执行动作的捕捉程度）、奖励信息量（即状态表征对所获奖励的描述程度）以及提示风格（或自然语言压缩程度，即对完整文本历史的概括水平）。\n\n我们将该框架应用于动态自私路由博弈——该博弈在理论和人类受试实验中均存在简单均衡\\cite{rapoport_choice_2009}。尽管该博弈结构相对简单，我们仍发现LLM智能体行为对自然语言状态表征存在关键依赖性。具体而言，当状态表征满足以下特征时，智能体行为更接近博弈论均衡预测且表现更稳定：(1)提供过去历史的概括性而非完整自然语言表征；(2)传递后悔信息而非原始收益数据；(3)仅包含有限的他方动作信息。相比之下，其他表征方式可能导致显著偏离均衡、动态博弈过程中更高波动性或两者兼有的情况。"
  },
  {
    "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning",
    "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.",
    "link": "http://arxiv.org/abs/2506.15606v1",
    "published": "2025-06-18T16:30:02+00:00",
    "summary_zh": "大语言模型（LLMs）已在实际应用中变得不可或缺。然而其广泛部署引发了重大安全隐忧，尤其在应对社会危害性问题时表现显著。尽管通过模型对齐技术已投入大量精力提升安全性，但经对齐的模型仍可能因后续微调而破坏其安全防护机制——即便新增训练数据看似无害。本文通过实证研究揭示：这种脆弱性源于LLM参数中与安全密切相关的低秩子空间对微调过程的高度敏感性。基于这一发现，我们提出一种无需训练的创新方法——低秩外推法（LoX），通过对齐LLM的安全子空间进行外推来增强安全鲁棒性。实验结果表明：LoX能显著提升模型抵御良性及恶意微调攻击的鲁棒性，同时保持对新任务的适应能力。例如，在面对良性或恶意微调攻击时，LoX可使攻击成功率（ASR）绝对降低11%至54%。通过分析参数空间的ASR分布特征，我们发现LoX的成功机理在于外推过程将LLM参数转移至更平坦的区域，从而降低了对扰动的敏感度。代码已开源：github.com/VITA-Group/LoX。"
  },
  {
    "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters",
    "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.",
    "link": "http://arxiv.org/abs/2506.15595v1",
    "published": "2025-06-18T16:10:17+00:00",
    "summary_zh": "多GPU并行计算已成为机器学习任务（尤其是大语言模型）的主流范式。为降低GPU间通信延迟，传统并行任务通常依据物理邻近性分配GPU资源。然而这一长期假设存在显著局限——特别是在带宽分布不均衡的大规模异构GPU集群中。本文提出LiteGD系统，这是一种基于全局视角的轻量级动态GPU调度方案。针对海量GPU拓扑信息存储难题，LiteGD采用计算感知设计，利用在采样数据上训练的轻量级Transformer网络。我们定制的网络结构设计同时保证了可迁移性与可扩展性。LiteGD还运用双向树搜索算法，在前序步骤生成的数据中寻找最优GPU调度方案，既能识别近优解又可降低搜索开销。我们在同构/异构互连的实际与模拟GPU集群中分别实现并评估LiteGD。实验结果表明：该系统在不同集群配置下能持续实现约90%的GPU带宽利用率（真实H100集群达80%），其性能显著优于传统默认调度和互连拓扑感知方法，尤其在大规模异构环境中优势突出。"
  },
  {
    "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models",
    "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.",
    "link": "http://arxiv.org/abs/2506.15568v1",
    "published": "2025-06-18T15:43:16+00:00",
    "summary_zh": "我们对大型语言模型（LLMs）在性别公平性方面进行了全面评估，重点关注其处理二元性别与非二元性别的能力。尽管先前研究主要聚焦于二元性别区分，我们创新性地提出了性别包容性公平指数（GIFI）——这一全新综合指标可量化LLMs对不同性别的包容程度。GIFI包含多层次评估体系：从基于给定性别代词的简单探测，到在不同性别假设下测试模型生成能力与认知行为的多个维度，从而揭示与不同性别标识相关联的偏见。我们运用GIFI对22个具有代表性的开源及商业LLMs（涵盖不同规模与功能）进行了广泛评估，发现这些模型在性别包容性方面存在显著差异。本研究凸显了提升LLMs包容性的重要性，为生成式模型未来实现性别公平提供了关键基准。"
  },
  {
    "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents",
    "summary": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
    "link": "http://arxiv.org/abs/2506.15567v1",
    "published": "2025-06-18T15:43:10+00:00",
    "summary_zh": "失效分析（FA）是一个高度复杂且知识密集型的过程。在FA实验室的计算基础设施中集成人工智能组件，有望实现多项任务的自动化，包括图像中不合格项的检测、从多样化数据源检索类似案例，以及基于标注图像生成报告。然而，随着部署的AI模型数量增加，如何将这些组件编排为与FA流程无缝集成的高效协同工作流成为关键挑战。\n\n本文研究了一种基于大语言模型（LLM）的规划代理（LPA）的设计与实现，旨在辅助FA工程师解决分析案例。该LPA将LLM与高级规划能力及外部工具调用功能相结合，能够自主处理复杂查询、从外部系统检索相关数据并生成人类可读的响应。评估结果表明，该代理在支持FA任务方面展现出良好的运行效能与可靠性。"
  },
  {
    "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction",
    "summary": "Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.",
    "link": "http://arxiv.org/abs/2506.15556v1",
    "published": "2025-06-18T15:29:02+00:00",
    "summary_zh": "大型语言模型（LLMs）被广泛应用于实时语音聊天应用中，通常与文本转语音（TTS）系统结合使用以生成音频响应。然而，其庞大的规模常导致用户输入结束与音频输出开始之间存在明显延迟，从而影响用户体验。当LLMs部署在计算能力有限的消费级硬件上作为单用户语音助手时，这种延迟尤为显著。我们发现该延迟主要源于LLMs生成首句所需的时间——而TTS系统需以逐句合成的方式将此首句作为输入来生成音频响应。为解决这一瓶颈，我们提出预测生成（PredGen）这一创新框架，通过在输入阶段进行推测解码来缓解甚至消除该延迟。PredGen能在用户仍在说话时生成候选响应，使系统能以最小延迟启动TTS处理。在Lmsys和MT-Bench数据集上的模拟实验表明，该方法能在广泛用例中将延迟有效降低约2倍，且仅在输入阶段产生极小的额外计算成本——这些计算资源原本处于闲置状态。"
  },
  {
    "title": "Lessons from Training Grounded LLMs with Verifiable Rewards",
    "summary": "Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.",
    "link": "http://arxiv.org/abs/2506.15522v1",
    "published": "2025-06-18T14:58:13+00:00",
    "summary_zh": "生成基于事实且可信的回答仍是大型语言模型（LLMs）面临的核心挑战。尽管基于引文验证的检索增强生成（RAG）技术展现出潜力，但指令微调模型在简单场景中仍频繁失效：遗漏明确答案、错误引用证据或在证据充足时拒绝回答。本研究探索了强化学习（RL）与内部推理机制如何增强LLMs的事实锚定能力。我们采用GRPO（群体相对策略优化）方法，通过可验证的基于结果的奖励机制训练模型，聚焦答案正确性、引文充分性与拒答质量三大维度，且无需依赖黄金推理轨迹或昂贵标注数据。在ASQA、QAMPARI、ELI5和ExpertQA等基准测试中的综合实验表明，融合推理增强的模型显著优于纯指令微调版本，尤其在处理不可回答问题和生成规范引文方面表现突出。采用两阶段训练方案（先优化答案与引文行为，再训练拒答机制）进一步通过稳定学习信号提升了事实锚定效果。此外，我们重新审视了基于GPT-4蒸馏的指令微调方法，发现将其与GRPO结合能显著提升长文本生成式问答任务的性能。总体而言，本研究证实了推理能力、分阶段优化及结果驱动型强化学习对于构建更可验证可靠LLMs的重要价值。"
  },
  {
    "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation",
    "summary": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.",
    "link": "http://arxiv.org/abs/2506.15513v1",
    "published": "2025-06-18T14:48:19+00:00",
    "summary_zh": "检索增强生成（RAG）已成为利用当前外部信息更新大语言模型（LLM）响应的常用策略。然而，模型仍可能依赖记忆中的训练数据，绕过检索证据并产生污染性输出。我们提出检索路径污染评分（RePCS）这一诊断方法，在无需访问模型或重新训练的情况下检测此类行为。该方法通过计算输出分布间的Kullback-Leibler（KL）散度，对比两条推理路径：(i)仅使用查询参数的路径；(ii)同时使用查询与检索上下文的增强路径。低散度表明检索上下文影响微弱，暗示存在记忆现象。该流程具有模型无关性，无需梯度或内部状态访问，仅需增加一次前向传播。我们进一步推导出PAC风格保证，将KL阈值与用户定义的误报率、漏报率相关联。在Prompt-WNQA基准测试中，RePCS取得0.918的ROC-AUC值，以6.5个百分点优势超越最强基线方法，同时在NVIDIA T4 GPU上保持低于4.7%的延迟开销。RePCS为验证RAG系统是否有效利用检索提供了轻量级黑盒保障机制，这对安全关键型应用尤为重要。"
  },
  {
    "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach",
    "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.",
    "link": "http://arxiv.org/abs/2506.15512v1",
    "published": "2025-06-18T14:47:59+00:00",
    "summary_zh": "大型语言模型为远程学习学生等教育活动环节带来了根本性变革。当前远程学习资源的检索方式在上下文语义深度方面存在不足，难以针对学生复杂查询提供全面信息。本研究提出一种创新方法，通过在LangChain框架中集成基于GPT的模型来增强远程学习检索功能。我们运用思维链推理与提示工程，以更直观高效的方式实现了该系统。所提出的框架重点提升检索结果的精确度与相关性，从而返回最符合学生需求的、具有语境丰富性的综合解释与学习资源。通过与典型大语言模型的效果对比评估，本研究表明该方法能显著提高用户满意度与学习成效。"
  },
  {
    "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling",
    "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.",
    "link": "http://arxiv.org/abs/2506.15498v1",
    "published": "2025-06-18T14:37:59+00:00",
    "summary_zh": "过程式或分步骤监督在提升大语言模型（LLMs）复杂多步推理能力方面发挥了关键作用。然而，实现高效、高质量的过程自动化标注仍是重大挑战。为此，我们提出\"参考引导评估的单次遍历标注法\"（SPARE），这是一种创新的结构化框架，通过将每个解题步骤与参考解中的一个或多个步骤对齐，并辅以显式推理评估，实现单次遍历的分步骤标注。研究表明，参考引导的步骤级评估能有效促进四个跨领域数据集（涵盖数学推理、多跳组合问答和空间推理）的过程监督。实验证明，相较于基线方法，SPARE在以下场景中能提升推理性能：（1）离线强化学习框架下微调模型以实现推理时贪婪解码；（2）训练奖励模型用于对多个大语言模型生成结果进行排序/聚合。此外，SPARE在具有挑战性的数学数据集上不仅取得具有竞争力的性能表现，其效率更是达到基于树搜索自动标注法的2.6倍——仅需后者38%的运行时间。我们已公开代码库及训练好的SPARE-PRM模型，以促进后续研究和结果复现。"
  },
  {
    "title": "Context-Informed Grounding Supervision",
    "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.",
    "link": "http://arxiv.org/abs/2506.15480v1",
    "published": "2025-06-18T14:13:56+00:00",
    "summary_zh": "大型语言模型（LLMs）通常需要借助外部知识来提供其参数中未编码的信息或减少幻觉现象。在此类场景中，我们期望模型能基于提供的上下文生成响应。然而已有研究表明，仅在推理阶段追加上下文并不能确保生成内容与上下文紧密关联。为此，我们提出上下文知情引导监督（CINGS）——这是一种后训练监督方法：在响应前添加相关上下文进行模型训练，同时仅对响应部分的标记计算损失而屏蔽上下文部分。实验表明，在文本和视觉领域中，经CINGS训练的模型比标准指令微调模型展现出更强的上下文关联性。文本领域测试显示，CINGS在11个信息检索数据集上优于其他训练方法，并能与推理阶段的引导技术形成互补；视觉-语言领域实验中，用CINGS训练模型替换视觉-语言模型的LLM主干架构后，在四个基准测试中显著降低了幻觉现象，同时保持生成内容的事实一致性。这种关联性的提升并未导致下游通用性能下降。最后我们分析了CINGS增强关联性的内在机制，发现该方法能引导模型先验知识和行为发生偏移，从而隐式强化模型对外部上下文的依赖程度。"
  },
  {
    "title": "Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning",
    "summary": "Medical report generation from imaging data remains a challenging task in\nclinical practice. While large language models (LLMs) show great promise in\naddressing this challenge, their effective integration with medical imaging\ndata still deserves in-depth exploration. In this paper, we present MRG-LLM, a\nnovel multimodal large language model (MLLM) that combines a frozen LLM with a\nlearnable visual encoder and introduces a dynamic prompt customization\nmechanism. Our key innovation lies in generating instance-specific prompts\ntailored to individual medical images through conditional affine\ntransformations derived from visual features. We propose two implementations:\nprompt-wise and promptbook-wise customization, enabling precise and targeted\nreport generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets\ndemonstrate that MRG-LLM achieves state-of-the-art performance in medical\nreport generation. Our code will be made publicly available.",
    "link": "http://arxiv.org/abs/2506.15477v1",
    "published": "2025-06-18T14:09:34+00:00",
    "summary_zh": "从医学影像数据生成诊断报告仍是临床实践中的挑战性任务。尽管大语言模型（LLMs）在应对这一挑战方面展现出巨大潜力，但其与医学影像数据的有效整合仍需深入探索。本文提出MRG-LLM——一种新型多模态大语言模型（MLLM），该模型将冻结的大语言模型与可学习的视觉编码器相结合，并引入动态提示定制机制。我们的核心创新在于：通过基于视觉特征的条件仿射变换，为每张医学影像生成特定实例的定制化提示。我们提出两种实现方案：提示级定制与提示簿级定制，从而实现精准定向的报告生成。在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医学报告生成任务中达到了最先进的性能表现。本研究代码将公开发布。"
  },
  {
    "title": "All is Not Lost: LLM Recovery without Checkpoints",
    "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.",
    "link": "http://arxiv.org/abs/2506.15461v1",
    "published": "2025-06-18T13:48:33+00:00",
    "summary_zh": "在分散且计算能力有限的节点（如多个本地实例）上训练大语言模型，可降低训练成本并推动模型民主化。这一过程中不可避免的挑战在于节点会因故障或运营商调度策略而频繁变动，导致模型某一部分（即某个训练阶段）丢失。传统故障恢复方案要么采用检查点机制——定期将完整模型副本存储至额外存储设备，要么实施冗余计算。这些方法即使在无故障情况下也会产生显著的通信和/或计算开销，且在大模型场景中扩展性欠佳。本文提出CheckFree高效恢复方法：当某训练阶段失效时，用其相邻最近阶段的加权平均值进行替代。与现有技术相比，CheckFree无需额外计算或存储资源。但由于采用相邻阶段平均机制，该方法仅能恢复中间阶段的故障。我们进一步扩展该方法为CheckFree+，通过乱序流水线执行来容忍首尾阶段的崩溃。借助乱序流水线技术，这些阶段的行为可由相邻阶段模拟，使得CheckFree+只需复制直接相邻阶段的权重即可实现恢复。为支持(de)嵌入层的恢复，CheckFree+将这些层复制到相邻阶段，仅需较小的存储开销。我们在模型规模从124M到1.5B的LLaMa系列模型上，针对不同故障频率进行了全面评估。在中低故障率（5-10%）场景下，CheckFree和CheckFree+在时钟时间收敛速度方面比检查点机制和冗余计算分别提升超过12%。两种方案均可通过代码库运行：https://github.com/gensyn-ai/CheckFree。"
  },
  {
    "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
    "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
    "link": "http://arxiv.org/abs/2506.15455v1",
    "published": "2025-06-18T13:35:47+00:00",
    "summary_zh": "近期大型语言模型（LLMs）在推理基准测试中展现出较高准确率。然而，目前尚不明确这些表现究竟源于真实推理能力，还是对训练集的统计记忆。受因果阶梯理论（Pearl，2009）及其三个层级（关联、干预与反事实）启发，本文提出RE-IMAGINE框架——该框架既能表征LLMs的推理能力层级结构，又包含可自动生成不同层级问题变体的自动化流程。通过在中间符号表征层面改造问题，RE-IMAGINE能生成海量仅靠记忆无法解决的问题。该框架具有普适性，可应用于数学、代码和逻辑等多个推理领域。我们在四个广泛使用的基准测试上验证该框架，评估多类LLMs时发现：当模型面对问题变体时性能会出现下降。这些评估结果表明过往表现存在对统计记忆的依赖，同时也为针对推理层级技能的深入研究开辟了新方向。"
  },
  {
    "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation",
    "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.",
    "link": "http://arxiv.org/abs/2506.15453v1",
    "published": "2025-06-18T13:33:34+00:00",
    "summary_zh": "记录代码片段对于明确开发者和用户需重点关注的关键区域至关重要。典型示例包括用法示例及其他应用程序编程接口（API），这对第三方库尤为重要。随着大语言模型（LLMs）的兴起，核心研究目标是探究开发者常用的描述类型，并评估大语言模型（本文以Llama为例）在描述生成方面的支持效果。我们采用包含185,412个软件包及1,024,579个代码片段的NPM代码片段数据集，从中选取400个代码片段（及其描述）作为样本。首先，人工分类发现原始描述中55.5%侧重基于示例的用法说明，该结果凸显了清晰文档的重要性——部分描述因细节不足难以传达意图。其次，大语言模型将79.75%的原始描述正确识别为\"示例\"类，与人工判断结果一致，显示出其泛化倾向。第三，相较于原始描述，生成描述的平均相似度得分为0.7173，表明具备相关性但仍有改进空间（低于0.9的得分意味着存在部分不相关内容）。研究结果表明：根据代码片段的具体用途，文档意图可能表现为使用说明、安装指南或面向任何库用户的描述性学习示例等不同形式。"
  },
  {
    "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need",
    "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.",
    "link": "http://arxiv.org/abs/2506.15451v1",
    "published": "2025-06-18T13:24:04+00:00",
    "summary_zh": "基于大语言模型的多智能体系统已在社会模拟与复杂任务求解领域展现出显著潜力。然而当前框架在系统架构设计、跨领域泛化能力及性能保障等方面面临关键挑战，尤其在任务复杂度与智能体数量增加时更为突出。我们提出AgentGroupChat-V2这一创新框架，通过三大核心技术突破应对这些挑战：(1) 采用分治策略的全并行架构，将用户查询分解为层级化任务森林结构，实现依赖管理与分布式并发处理；(2) 自适应协作引擎能根据任务特征动态选择异构大语言模型组合及交互模式；(3) 结合分治方法的智能体组织优化策略，实现高效问题分解。大量实验表明，AgentGroupChat-V2在多元领域表现卓越——GSM8K测试准确率达91.50%（超越最佳基线5.6个百分点），竞赛级AIME测试准确率30.4%（近乎其他方法两倍），HumanEval测试pass@1指标达79.20%。随着任务难度提升，其性能优势愈发显著，在Level 5 MATH问题上较最先进基线提升超过11个百分点。这些结果证实，AgentGroupChat-V2为构建高效通用的大语言模型多智能体系统提供了全面解决方案，在复杂推理场景中具有显著优势。代码已开源：https://github.com/MikeGu721/AgentGroupChat-V2。"
  },
  {
    "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.",
    "link": "http://arxiv.org/abs/2506.15415v1",
    "published": "2025-06-18T12:35:53+00:00",
    "summary_zh": "大型语言模型（LLMs）虽已展现出卓越能力，但在斯瓦希里语等低资源语言（LRLs）中的表现往往因数据稀缺及预训练阶段代表性不足而逊色。实现稳健的跨语言词汇对齐成为翻译、跨语言信息检索等任务的关键挑战。本文提出\"靶向词汇注入\"（TLI）这一新颖高效的微调方法。\n\n我们首先验证了以斯瓦希里语为核心的Lugha-Llama-8B-wura模型，在其初始内部层（具体为第2层，基于预实验的平均余弦相似度达0.99998）已具备近乎完美的斯瓦希里语-英语词汇对齐能力，但该特性未充分体现在最终输出表征中（基线评估集相似度仅约0.32）。TLI方法利用这一发现，通过低秩自适应（LoRA）和对比学习目标进行微调，专门针对该经实证确定的最佳初始层嵌入进行优化。\n\n实验表明，TLI显著提升了623组训练词汇对的输出层对齐效果，平均余弦相似度从0.3211提升至0.4113（增幅28.08%，p<1.33×10^-240）。更重要的是，这种提升能极好地泛化至63组未见的对照词汇对，相似度从0.3143提升至0.4033（增幅28.32%，p<7.17×10^-27）。这些发现证明TLI能有效增强模型保持并传递其固有初始层跨语言知识的能力，为提升低资源语言大模型的词汇对齐提供了参数高效且切实有效的解决方案。"
  },
  {
    "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture",
    "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.",
    "link": "http://arxiv.org/abs/2506.15355v1",
    "published": "2025-06-18T11:19:25+00:00",
    "summary_zh": "语言模型（LMs）是塑造现代工作流程不可或缺的工具，但其全球适用性取决于对地方社会文化背景的理解。为此，我们推出SANSKRITI基准测试，旨在评估语言模型对印度丰富文化多样性的认知能力。该基准包含21,853组精心编纂的问答对，覆盖印度28个邦和8个联邦属地，是目前测试印度文化知识规模最大的数据集。SANSKRITI涵盖印度文化的十六项核心要素：仪式典礼、历史、旅游、饮食、舞蹈音乐、服饰、语言、艺术、节庆、宗教、医药、交通、体育、夜生活及名人，全面展现了印度文化的多元图景。通过在主流大语言模型（LLMs）、印度语言模型（ILMs）及小型语言模型（SLMs）上的评估，我们发现这些模型在处理文化细微差异类查询时存在显著能力差异，许多模型在特定区域语境中表现欠佳。SANSKRITI凭借其海量、文化底蕴深厚且多元的数据集，为评估和提升语言模型的文化理解能力设立了新标准。"
  },
  {
    "title": "DeVisE: Behavioral Testing of Medical Large Language Models",
    "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.",
    "link": "http://arxiv.org/abs/2506.15339v1",
    "published": "2025-06-18T10:42:22+00:00",
    "summary_zh": "大型语言模型（LLMs）在临床决策支持中的应用日益广泛，但现有评估方法往往难以区分真正的医学推理与表面模式。我们提出DeVisE（人口统计学与生命体征评估）这一行为测试框架，用于探究细粒度的临床理解能力。我们基于MIMIC-IV数据库构建了ICU出院记录数据集，通过控制针对人口统计学特征（年龄、性别、种族）和生命体征属性的单变量反事实变量，生成原始（真实世界）和模板化（合成）两种版本数据。我们在零样本和微调两种设置下，评估了五种涵盖通用型和医学专用型的大型语言模型。通过以下两个维度衡量模型行为：（1）输入层敏感性——反事实变量如何改变病历生成概率；（2）下游推理——这些变量如何影响预测住院时长。结果表明：零样本模型展现出更连贯的反事实推理模式，而微调模型虽然稳定性更强，但对临床意义变化的响应性较弱。值得注意的是，人口统计学因素会以微妙但一致的方式影响输出结果，这凸显了公平性评估的重要性。本研究证明了行为测试在揭示临床LLMs推理策略方面的价值，可为设计更安全、更透明的医疗AI系统提供参考。"
  },
  {
    "title": "Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests",
    "summary": "Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.",
    "link": "http://arxiv.org/abs/2506.15330v1",
    "published": "2025-06-18T10:10:02+00:00",
    "summary_zh": "临床检验结果在各类疾病诊断中普遍存在。基于已检测项目的结果预测未开具检查项目的异常值颇具研究价值，因为这有望实现全民早期诊断。其中血常规（CBC）检测因其临床应用最广泛而占据特殊地位。将常规生化检验项目与血常规结合后，会形成因人而异的检验值配对集合——或在常规场景下表现为含缺失值的表格数据。本研究将表格建模问题构建为集合转换问题：源集合由类GPT标签列嵌入及其对应值组成，目标集合则仅包含同类嵌入。该方法无需隐式估计即可有效处理缺失值，实现了大语言模型领域与表格数据领域的跨界融合。应用于临床检验数据后，在联合预测高尿酸、高血糖、高胆固醇及低铁蛋白水平方面，AUC指标最高提升达8%。"
  },
  {
    "title": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes",
    "summary": "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.",
    "link": "http://arxiv.org/abs/2506.15307v1",
    "published": "2025-06-18T09:36:57+00:00",
    "summary_zh": "大型语言模型（LLMs）已深刻改变了众多领域，然而其在医疗、金融等隐私敏感领域的专业任务适配却受限于严格隐私要求导致的训练数据匮乏。基于安全多方计算（MPC）的隐私保护机器学习为同时保护模型参数与用户数据提供了有效方案，但其在LLMs中的应用主要局限于推理阶段——因为微调会带来显著的计算挑战，尤其在隐私保护的反向传播和优化器操作方面。本文揭示了基于MPC的LLMs隐私保护微调两大核心障碍：（1）反向传播与优化器过程的巨大计算开销；（2）MPC环境下基于softmax的注意力机制效率低下。针对这些挑战，我们提出首个面向高效隐私保护LLMs微调的MPC框架SecFwT。该框架创新性地采用仅前向传播的调参范式以消除反向传播与优化器计算，并运用MPC友好的随机特征注意力机制近似softmax注意力，大幅减少高成本非线性运算与计算复杂度。实验结果表明，SecFwT在效率与隐私保护方面均实现显著提升，为隐私关键应用中的LLMs可扩展安全微调提供了可行方案。"
  },
  {
    "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification",
    "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.",
    "link": "http://arxiv.org/abs/2506.15304v1",
    "published": "2025-06-18T09:35:33+00:00",
    "summary_zh": "语言识别（LID）是从网络爬取数据中整理多语言大语言模型预训练语料的关键步骤。尽管许多关于LID模型训练的研究侧重于收集多样化训练数据以提升性能，但低资源语言——通常仅限于单一领域数据（如《圣经》）——的表现仍然欠佳。为解决这些类别不平衡与偏差问题，我们提出了一种新颖的监督对比学习（SCL）方法，用于学习低资源语言的领域不变表征。通过大量分析表明，该方法使低资源语言在领域外数据上的LID性能提升了3.2%，验证了其在增强LID模型方面的有效性。"
  },
  {
    "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment",
    "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.",
    "link": "http://arxiv.org/abs/2506.15301v1",
    "published": "2025-06-18T09:32:16+00:00",
    "summary_zh": "大语言模型（LLMs）的最新进展显著提升了通用领域自然语言处理任务的性能。然而，在临床试验招募等关键领域的应用仍十分有限。由于临床试验设计以自然语言表述，患者数据同时包含结构化与非结构化文本，试验与患者的匹配任务能从大语言模型的知识聚合与推理能力中获益。传统方法具有试验特异性，而具备分布式知识整合能力的大语言模型有望构建更普适的解决方案。但近期基于大语言模型的应用多依赖专有模型，且缺乏强有力的评估基准。本综述首次对试验-患者匹配任务进行分析，并将新兴的大语言模型方法置于临床试验招募的背景下进行探讨。我们系统评述了现有基准、方法及评估框架，深入分析了大语言模型技术在临床研究应用中的挑战，并展望了极具前景的未来发展方向。"
  },
  {
    "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data",
    "summary": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.",
    "link": "http://arxiv.org/abs/2506.15271v1",
    "published": "2025-06-18T08:46:59+00:00",
    "summary_zh": "大型语言模型（LLMs）的卓越能力主要归功于其海量训练数据集——这些数据通常未经数据所有者许可而从互联网抓取，严重侵犯知识产权。数据集推断（DI）技术通过识别可疑数据集是否被用于训练，为数据所有者验证未授权使用提供了潜在解决方案。然而现有DI方法需要一个确知未被纳入训练的私有数据集作为基准，且该数据集需与泄露数据分布高度匹配。实践中这种同分布保留数据极难获取，极大限制了DI技术的适用性。本研究通过合成生成所需保留数据集来攻克这一难题，重点解决两大关键挑战：（1）构建能精准反映原始分布的高质量多样化合成数据，我们通过基于后缀补全任务的专用数据生成器实现；（2）弥合真实数据与合成数据间的似然差异，采用事后校准技术完成。在多种文本数据集上的大量实验表明，使用我们生成的保留数据集可使DI技术高置信度检测出原始训练集，同时保持极低的误报率。该成果使版权所有者能对数据使用提出合法主张，验证了本方法在实际诉讼中的可靠性。代码已开源：https://github.com/sprintml/PostHocDatasetInference。"
  },
  {
    "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments",
    "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.",
    "link": "http://arxiv.org/abs/2506.15253v1",
    "published": "2025-06-18T08:30:36+00:00",
    "summary_zh": "大型语言模型（LLM）智能体在医疗、金融等关键领域的快速部署亟需建立完善的安全框架。针对当前动态环境中LLM智能体缺乏标准化评估基准的问题，我们提出RAS-Eval综合安全评测体系，支持模拟环境与真实工具执行的双重测试。该基准包含80个测试案例及3,802项攻击任务，覆盖11类常见软件弱点枚举（CWE），工具实现形式涵盖JSON、LangGraph及模型上下文协议（MCP）三种标准。通过对6种前沿LLM在多样化场景下的评估，我们发现显著安全隐患：攻击行为平均导致智能体任务完成率（TCR）下降36.78%，在学术环境中攻击成功率高达85.65%。值得注意的是，安全能力同样遵循规模定律——更大规模的模型展现出更优的防护表现。本研究揭示了实际部署中LLM智能体的重大风险，并为未来安全研究奠定了基准框架。代码与数据集已开源发布于https://github.com/lanzer-tree/RAS-Eval。"
  },
  {
    "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge",
    "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.",
    "link": "http://arxiv.org/abs/2506.15246v1",
    "published": "2025-06-18T08:24:27+00:00",
    "summary_zh": "我们提出TopClustRAG——一个专为LiveRAG挑战赛开发的检索增强生成（RAG）系统，该赛事旨在评估面向大规模网络语料库的端到端问答能力。本系统采用融合稀疏索引与稠密索引的混合检索策略，并通过K-Means聚类对语义相似的文本段落进行分组。随后从每个聚类中选取代表性段落构建特定于该聚类的提示模板，驱动大语言模型（LLM）生成中间答案，再经过滤、重排序后最终合成单一综合响应。这种多阶段处理流程显著提升了答案的多样性、相关性及对检索证据的忠实度。在FineWeb Sample-10BT数据集上的评估显示，TopClustRAG在官方排行榜中以忠实度排名第二、正确性位列第七，验证了基于聚类的上下文过滤与提示聚合方法在大规模RAG系统中的有效性。"
  },
  {
    "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants",
    "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.",
    "link": "http://arxiv.org/abs/2506.15239v1",
    "published": "2025-06-18T08:20:19+00:00",
    "summary_zh": "本文评估了当前语言技术在理解巴斯克语和西班牙语变体方面的能力。我们以自然语言推理（NLI）作为核心任务，构建了一个全新的人工标注平行语料库，涵盖巴斯克语、西班牙语及其各自变体。通过对仅编码器型和大语言模型（LLM）的跨语言及上下文学习实验进行实证分析，发现模型在处理语言变体时（尤其是巴斯克语）存在性能下降现象。错误分析表明这种性能衰退并非源于词汇重叠，而是语言变体本身所致。进一步的消融实验显示，仅编码器模型在处理西巴斯克语时表现尤为困难，这与语言学理论中关于边缘方言（如西巴斯克语）与标准语差异更大的论断相符。所有数据与代码均已公开。"
  },
  {
    "title": "Large Language Models for Unit Testing: A Systematic Literature Review",
    "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.",
    "link": "http://arxiv.org/abs/2506.15227v1",
    "published": "2025-06-18T08:11:10+00:00",
    "summary_zh": "单元测试是现代软件工程中的基础实践，旨在确保单个软件组件的正确性、可维护性和可靠性。近年来，随着大语言模型（LLMs）的快速发展，大量研究开始利用LLMs自动化各类单元测试任务，展现出卓越性能并显著减少了人工投入。然而，由于基于LLM的单元测试领域仍处于探索阶段，研究人员难以全面把握现有成果、待解决问题及未来机遇。本文首次系统梳理了截至2025年3月LLMs在单元测试中的应用研究，从单元测试和大语言模型双重视角分析了\\numpaper{}篇相关文献。我们首先对受益于LLMs的现有单元测试任务（如测试用例生成和预言生成）进行分类；继而探讨将LLMs融入单元测试研究的关键维度，包括模型选用、适配策略及混合方法；进一步总结了尚未解决的核心挑战，并指明未来研究方向。本研究为单元测试学界提供了系统性的研究图谱，助力学者全面认知领域进展并推动后续探索。相关研究资料已公开发布于GitHub仓库：https://github.com/iSEngLab/AwesomeLLM4UT。"
  },
  {
    "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models",
    "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.",
    "link": "http://arxiv.org/abs/2506.15220v1",
    "published": "2025-06-18T07:58:41+00:00",
    "summary_zh": "视频蕴含着丰富的信息，而用自然语言生成详细准确的描述是视频理解的关键环节。本文提出video-SALMONN 2——一种采用低秩自适应（LoRA）技术的先进视听大语言模型（LLM），通过定向偏好优化（DPO）实现带配对音频的视频字幕生成能力提升。我们提出了评估视频描述完整性与准确性的新指标，并基于DPO进行优化。为进一步改进训练过程，我们创新性地提出多轮DPO（MrDPO）方法：在每轮训练（1000步）后定期更新DPO参考模型，将LoRA模块合并重置作为参数更新的代理机制，并引入真实视频字幕的引导信号以稳定训练过程。实验结果表明，MrDPO显著提升了video-SALMONN 2的字幕生成准确率，使错误率降低28%。最终仅含70亿参数的video-SALMONN 2模型，在视频字幕任务中超越GPT-4o和Gemini-1.5-Pro等领先模型，同时在同类规模模型的主流视频问答基准测试中保持极具竞争力的性能表现。代码已开源：\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}。"
  },
  {
    "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs",
    "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.",
    "link": "http://arxiv.org/abs/2506.15215v1",
    "published": "2025-06-18T07:49:13+00:00",
    "summary_zh": "开放式问答（QA）是评估大语言模型（LLMs）能力的关键任务。与封闭式问答相比，它要求更长的答案陈述、更细致的推理过程以及多样化的表达方式，这使得构建精细且可解释的自动评估体系既至关重要又充满挑战。传统指标如ROUGE和BERTScore由于模型回复与参考答案之间存在模式差异，难以捕捉语义相似性。当前基于大语言模型的评估方法（如候选答案的两两比较或列表排序）缺乏直观的可解释性。虽然对每个回复进行逐点评分能提供一定描述，但无法适应不同问题内容。最值得注意的是，现有方法忽视了事实型与非事实型问题的区分。为应对这些挑战，我们提出\\textbf{MinosEval}这一新型评估方法——该方法首先区分问题类型，继而采用差异化评估策略对候选答案进行排序：针对事实型问题应用自适应关键点评分策略，针对非事实型问题则采用实例感知的列表排序策略。在多个开放式问答数据集（包括为补充社区资源而构建的含更多候选回复的自建数据集）上的实验表明，MinosEval与人工标注的吻合度更高，且能提供更具可解释性的评估结果。"
  },
  {
    "title": "LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis",
    "summary": "With the rapid advancements in Natural Language Processing (NLP), large\nlanguage models (LLMs) like GPT-4 have gained significant traction in diverse\napplications, including security vulnerability scanning. This paper\ninvestigates the efficacy of GPT-4 in identifying software vulnerabilities\ncompared to traditional Static Application Security Testing (SAST) tools.\nDrawing from an array of security mistakes, our analysis underscores the potent\ncapabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that\nGPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in\ndetecting 32 types of exploitable vulnerabilities. This study also addresses\nthe potential security concerns surrounding LLMs, emphasising the imperative of\nsecurity by design/default and other security best practices for AI.",
    "link": "http://arxiv.org/abs/2506.15212v1",
    "published": "2025-06-18T07:47:12+00:00",
    "summary_zh": "随着自然语言处理（NLP）技术的快速发展，以GPT-4为代表的大语言模型（LLMs）已在包括安全漏洞扫描在内的多个领域获得广泛应用。本文通过对比传统静态应用程序安全测试（SAST）工具，研究了GPT-4在识别软件漏洞方面的效能。基于多类安全错误样本的分析，我们揭示了GPT-4在LLM增强型漏洞扫描中的强大能力。研究发现，在检测32类可利用漏洞时，GPT-4（高级数据分析版）的准确率较SAST工具高出94%。本研究同时探讨了LLMs潜在的安全隐患，强调了\"安全设计/默认\"原则及其他人工智能安全最佳实践的必要性。"
  },
  {
    "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs",
    "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.",
    "link": "http://arxiv.org/abs/2506.15211v1",
    "published": "2025-06-18T07:44:09+00:00",
    "summary_zh": "近期基于长链式思维（Long CoT）训练的大规模推理模型（LRMs）所取得的进展，已展现出卓越的跨领域泛化能力。然而支撑这种迁移能力的底层机制仍不为人知。我们提出假说：跨领域泛化源于共享的抽象推理原型——这些基本推理模式能够捕捉跨领域问题的本质特征。这些原型通过最小化表征差异，揭示出表面迥异的任务实则建立在共通的推理结构之上。基于此假说，我们提出ProtoReasoning框架，通过可扩展且可验证的原型表征（逻辑推理采用Prolog、规划任务采用PDDL）来增强大语言模型的推理能力。该框架具有三大特性：(1) 自动化原型构建流水线，可将问题转化为对应原型表征；(2) 完备的验证系统，通过Prolog/PDDL解释器提供可靠反馈；(3) 在保证正确性的前提下，具备在原型空间内任意合成问题的可扩展性。大量实验表明：ProtoReasoning在逻辑推理（Enigmata-Eval）上较基线模型提升4.7%，规划任务提升6.3%，通用推理（MMLU）提升4.0%，数学推理（AIME24）提升1.0%。值得注意的是，消融研究证实：相较于仅在自然语言表征上训练，原型空间中的学习能显著增强对结构相似问题的泛化能力，这验证了我们关于\"推理原型是大语言模型可泛化推理基础的\"核心假说。"
  },
  {
    "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals",
    "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).",
    "link": "http://arxiv.org/abs/2506.15208v1",
    "published": "2025-06-18T07:42:32+00:00",
    "summary_zh": "2012年，联合国提出了17项可持续发展目标（SDGs），旨在到2030年创造一个更具可持续性和更美好的未来。然而，由于相关数据规模庞大且结构复杂，追踪这些目标的进展十分困难。文本分类模型已成为该领域的重要工具，能够自动分析来自多种来源的海量文本。此外，得益于识别复杂语言模式和语义的能力，大型语言模型（LLMs）近年来已被证明对包括文本分类在内的诸多自然语言处理任务不可或缺。本研究针对聚焦于SDGs的单标签多类文本分类任务，分析了各类专有和开源大型语言模型的表现。同时，该研究还评估了任务适应技术（即上下文学习方法）——包括零样本学习、少样本学习以及微调——在该领域的有效性。结果表明，经过提示工程优化的小型模型，其性能可与OpenAI的GPT（生成式预训练变换器）等大型模型相媲美。"
  },
  {
    "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges",
    "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
    "link": "http://arxiv.org/abs/2506.15196v1",
    "published": "2025-06-18T07:20:01+00:00",
    "summary_zh": "启发式算法在解决组合优化（CO）问题中发挥着关键作用，但传统设计高度依赖人工经验且难以跨实例泛化。我们提出**HeurAgenix**——一个由大语言模型（LLM）驱动的两阶段超启发式框架，可先演化启发式策略再自动择优。在启发式演化阶段，HeurAgenix利用LLM对比种子启发式解与高质量解，提取可复用的演化策略；在问题求解时，通过LLM的感知能力动态选择最适合当前问题状态的启发式方法。该选择器既可采用前沿LLM，也可使用推理成本更低的轻量级微调模型以保证灵活性。针对CO复杂性导致的可靠监督信号稀缺问题，我们采用双奖励机制微调轻量级启发式选择器，联合利用选择偏好信号与状态感知信号，在噪声标注条件下实现鲁棒选择。在经典基准测试中的大量实验表明，HeurAgenix不仅超越现有基于LLM的超启发式方法，其性能更能达到或超过专用求解器。代码已开源：https://github.com/microsoft/HeurAgenix。"
  },
  {
    "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem",
    "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.",
    "link": "http://arxiv.org/abs/2506.15170v1",
    "published": "2025-06-18T06:33:19+00:00",
    "summary_zh": "大型语言模型（LLMs）正迅速从单模态系统发展为多模态大语言模型及智能体，在显著扩展功能的同时也引入了日益严峻的安全风险。本文系统梳理了LLM生态体系扩张过程中越狱攻击手段与防御机制的复杂化演变。我们首先追溯从LLM到多模态大语言模型（MLLMs）及智能体的发展轨迹，重点剖析各阶段涌现的核心安全挑战。其次从攻击影响力和可见性双重视角对主流越狱技术进行分类，全面解析代表性攻击方法、相关数据集及评估指标。在防御层面，我们基于响应时机和技术路径对现有策略进行体系化归类，阐明其适用性与实施框架。研究进一步指出当前综述存在的核心缺陷：对智能体特有安全问题关注不足、混合越狱方法缺乏明确分类体系、实验设置分析不够详尽、近期进展覆盖滞后等。针对这些局限，我们整合最新研究成果，提出数据集构建、评估框架优化及策略泛化等未来研究方向。本研究旨在深化对越狱机制的认知，推动面向能力持续进化的大语言模型构建更具韧性与适应性的防御体系。"
  },
  {
    "title": "LLM Agent for Hyper-Parameter Optimization",
    "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.",
    "link": "http://arxiv.org/abs/2506.15167v1",
    "published": "2025-06-18T06:28:22+00:00",
    "summary_zh": "超参数对通信算法性能至关重要。然而，当前针对具备无线电地图功能的无人机（UAV）轨迹与通信场景中采用交叉变异预热粒子群优化（WS-PSO-CM）算法的超参数调优方法主要基于启发式规则，存在自动化程度低、性能欠佳等问题。本文设计了一种基于大语言模型（LLM）的自动超参数调优代理，采用迭代框架与模型上下文协议（MCP）实现调优过程。具体而言：首先通过配置文件设定LLM代理的任务背景与输出格式；随后代理根据提示需求驱动，迭代调用WS-PSO-CM算法进行探索；最终自主终止循环并输出最优超参数组合。实验结果表明，本方法生成的超参数所实现的最小和速率显著优于人工启发式方法与随机生成方法，证实具备PSO知识储备与WS-PSO-CM算法背景的LLM代理能有效发掘高性能超参数组合。"
  },
  {
    "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation",
    "summary": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.",
    "link": "http://arxiv.org/abs/2506.15157v1",
    "published": "2025-06-18T06:02:06+00:00",
    "summary_zh": "模仿学习（IL）旨在通过观察少量人类示范动作，使机器人能够自主完成任务。近期出现的情境化模仿学习（In-Context IL）变体，利用现成大语言模型（LLMs）作为即时策略——这些模型能从给定少量示范中理解情境来执行新任务，而无需通过大规模示范显式更新网络模型。然而，基于LLM的即时策略存在幻觉问题（如偶尔生成偏离示范的低质量轨迹），这削弱了其在机器人领域的可靠性。为缓解该问题，我们提出一种新型鲁棒情境化模仿学习算法——鲁棒即时策略（RIP）。该算法采用学生t回归模型来抵御即时策略产生的幻觉轨迹，从而实现可靠轨迹生成。具体而言，RIP通过LLM生成多个候选机器人轨迹来完成给定任务，并利用学生t分布对这些轨迹进行聚合（该分布能有效忽略异常值即幻觉轨迹），最终生成抗幻觉的鲁棒轨迹。我们在仿真与真实环境中的实验表明：RIP显著优于现有最优IL方法，在日常任务的低数据场景中任务成功率至少提升26%。视频结果详见https://sites.google.com/view/robustinstantpolicy。"
  },
  {
    "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
    "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
    "link": "http://arxiv.org/abs/2506.15155v1",
    "published": "2025-06-18T05:56:01+00:00",
    "summary_zh": "大型语言模型在数据中心的部署日益广泛。为这些模型提供服务需要精细的内存管理，因为其内存占用包含静态权重、动态激活值以及键值缓存三部分。虽然静态权重恒定且可预测，但激活值和KV缓存等动态组件在运行时频繁变化，给高效内存管理带来重大挑战。现代大语言模型服务系统通常采用分层架构处理运行时内存与KV缓存：运行时内存管理基于静态张量抽象实现，而KV缓存则通过构建于张量抽象之上的页表式虚拟化层进行管理。这种虚拟化技术能动态调控KV缓存以缓解内存碎片问题。然而，这种双层分离的设计从根本上割裂了运行时内存与KV缓存的管理机制，导致动态工作负载下内存利用率低下，可能引发近20%的吞吐量下降。\n\n针对这些局限，我们提出eLLM弹性内存管理框架，其设计灵感源自操作系统的经典内存气球机制。eLLM的核心组件包括：(1) 虚拟张量抽象——将张量的虚拟地址空间与物理GPU内存解耦，构建统一灵活的内存池；(2) 弹性内存机制——通过运行时内存膨胀与收缩动态调整内存分配，并利用CPU内存作为可扩展缓冲区；(3) 轻量级调度策略——采用SLO感知策略优化内存利用率，在严格的服务等级目标约束下有效平衡性能权衡。全面评估表明，eLLM显著优于现有最优系统：解码吞吐量提升2.32倍，且能支持3倍于基线的大批量（128K-token）输入处理。"
  },
  {
    "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs",
    "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.",
    "link": "http://arxiv.org/abs/2506.15131v1",
    "published": "2025-06-18T04:19:33+00:00",
    "summary_zh": "开放域对话（OD）具有\"一对多\"（o2m）特性，即单个对话上下文可对应多个合理回复。尽管已有研究表明建模该特性能提升回复多样性，但当前大多数基于大语言模型（LLM）的对话代理并未显式实现这一点。本研究通过将OD生成分解为两大核心任务——多回复生成（MRG）与偏好选择（PS）——来建模LLM中的o2m特性：前者需针对给定对话上下文生成n个语义与词汇层面高度多样化的优质回复，后者则需基于人类偏好从中筛选最优回复。为此我们构建了专门捕捉o2m特性的对话语料库o2mDial，该语料库为每个上下文配置了多个合理回复选项。基于o2mDial，我们提出了新型上下文学习与指令微调策略、创新的MRG评估指标，以及面向PS的模型化解决方案。实验表明，将该两阶段框架应用于小型LLM进行OD生成时，能在保持上下文连贯性的同时显著提升回复多样性（最高使回复质量提升90%），使其性能接近更大规模模型。"
  },
  {
    "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models",
    "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.",
    "link": "http://arxiv.org/abs/2506.15098v1",
    "published": "2025-06-18T03:08:04+00:00",
    "summary_zh": "增强报告（ERs）作为用户与开发者之间的关键沟通渠道，承载着软件改进的宝贵建议。然而人工处理这些报告资源消耗巨大，易导致延误并可能造成重要见解流失。为应对这一挑战，增强报告审批预测（ERAP）已成为研究热点，其通过机器学习技术实现决策自动化。传统方法多采用基于特征的分类器和深度学习模型，而大语言模型（LLM）的最新进展为提升预测精度带来了新机遇。本研究系统评估了18种LLM变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA和XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct及DeepSeek-V3等解码器模型）相较于传统方法（CNN/LSTM-BERT/GloVe）的表现。实验揭示两大关键发现：（1）引入创建者资料可使未经微调的纯解码器模型整体准确率提升10.8%，但可能引入偏差；（2）经LoRA微调的Llama 3.1 8B Instruct进一步优化性能，达到79%准确率，且显著提升已批准报告的召回率（76.1%对比LSTM-GLOVE的64.1%），在严格时序评估下以5%优势超越传统方法，并有效解决类别不平衡问题。这些发现确立了LLM作为ERAP更优解决方案的地位，展现出其简化软件维护流程、提升实际开发环境中决策能力的潜力。我们还系统考察并总结了大型模型表现欠佳的ER案例，为未来研究提供了重要方向指引。"
  },
  {
    "title": "Make Your AUV Adaptive: An Environment-Aware Reinforcement Learning Framework For Underwater Tasks",
    "summary": "This study presents a novel environment-aware reinforcement learning (RL)\nframework designed to augment the operational capabilities of autonomous\nunderwater vehicles (AUVs) in underwater environments. Departing from\ntraditional RL architectures, the proposed framework integrates an\nenvironment-aware network module that dynamically captures flow field data,\neffectively embedding this critical environmental information into the state\nspace. This integration facilitates real-time environmental adaptation,\nsignificantly enhancing the AUV's situational awareness and decision-making\ncapabilities. Furthermore, the framework incorporates AUV structure\ncharacteristics into the optimization process, employing a large language model\n(LLM)-based iterative refinement mechanism that leverages both environmental\nconditions and training outcomes to optimize task performance. Comprehensive\nexperimental evaluations demonstrate the framework's superior performance,\nrobustness and adaptability.",
    "link": "http://arxiv.org/abs/2506.15082v1",
    "published": "2025-06-18T02:47:51+00:00",
    "summary_zh": "本研究提出了一种新型环境感知强化学习（RL）框架，旨在增强自主水下航行器（AUV）在水下环境中的作业能力。该框架突破传统RL架构，创新性地整合了环境感知网络模块，可动态捕获流场数据，并将这一关键环境信息有效嵌入状态空间。这种集成实现了实时环境适应，显著提升了AUV的态势感知与决策能力。此外，框架将AUV结构特性纳入优化过程，采用基于大语言模型（LLM）的迭代优化机制，通过融合环境条件与训练成果来优化任务性能。全面实验评估表明该框架具有卓越的性能、鲁棒性和适应性。"
  },
  {
    "title": "Learning-Time Encoding Shapes Unlearning in LLMs",
    "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.",
    "link": "http://arxiv.org/abs/2506.15076v1",
    "published": "2025-06-18T02:42:02+00:00",
    "summary_zh": "随着大语言模型（LLMs）在现实世界中的部署日益广泛，出于隐私法规、修正过时或有害内容等多重考量，\"遗忘\"（即事后移除特定知识）能力已成为关键需求。既有研究提出了遗忘基准测试与算法方案，但通常假设训练过程及目标模型固定不变。本研究通过实证方法探究了知识编码阶段的学习决策如何影响事实知识遗忘效果。实验揭示两大核心发现：（1）采用释义描述进行学习能提升遗忘性能；（2）从文本片段中遗忘单个知识点具有显著难度。结果表明，学习阶段的知识编码方式可能是实现可靠事后遗忘的关键因素。"
  },
  {
    "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation",
    "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
    "link": "http://arxiv.org/abs/2506.15068v1",
    "published": "2025-06-18T02:16:53+00:00",
    "summary_zh": "评估开放式长文本生成具有挑战性，因为难以明确定义何为优质输出与劣质输出的分界。现有方法往往忽略连贯性、风格或相关性等关键维度，或受预训练数据偏差影响，导致开放式长文本评估成为尚未充分探索的领域。为填补这一空白，我们提出PrefBERT评分模型，用于在GRPO框架中评估开放式长文本生成，并通过区分优劣输出的差异化奖励机制指导其训练。该模型基于两个包含多样化长文本风格及Likert量表质量评级的响应评估数据集进行训练，能比传统指标ROUGE-L和BERTScore提供更优的语义奖励反馈，从而有效支持GRPO系统。通过包括大语言模型裁判、人工评分及定性分析在内的综合评估表明：基于多句级和段落级响应训练的PrefBERT，在处理不同篇幅的长文本时仍保持可靠性，并与GRPO所需的可验证奖励机制高度契合。人工评估证实，采用PrefBERT作为奖励信号训练策略模型所产生的响应，比使用传统指标训练的版本更符合人类偏好。代码已开源：https://github.com/zli12321/long_form_rl。"
  },
  {
    "title": "ChatModel: Automating Reference Model Design and Verification with LLMs",
    "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.",
    "link": "http://arxiv.org/abs/2506.15066v1",
    "published": "2025-06-18T02:15:02+00:00",
    "summary_zh": "随着集成电路设计复杂度的持续攀升，功能验证的难度日益加大。作为加速验证流程的关键环节，参考模型本身的开发也变得愈发复杂耗时。尽管大语言模型（LLM）在代码编程领域展现出潜力，但高效生成复杂参考模型仍是重大挑战。为此，我们推出首个基于大语言模型辅助的敏捷参考模型生成与验证平台——ChatModel。该平台通过整合设计标准化与分层敏捷建模方法，实现了从设计规范到完整功能参考模型的流程简化。采用模块化生成策略不仅提升了LLM生成参考模型的设计能力，更显著提高了验证效率。我们在300个不同复杂度的设计案例上评估ChatModel，结果显示其在参考模型生成的效率和质量方面均有显著提升。相较于其他方法，ChatModel实现了55.02%的最高性能改进，在生成稳定性方面表现突出，并使参考模型设计方案产出能力提升9.18倍。此外，与传统方法相比，它将参考模型设计与验证的迭代过程平均加速了5.90倍。这些成果凸显了ChatModel在推动参考模型生成与验证自动化进程中的巨大潜力。"
  },
  {
    "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models",
    "summary": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.",
    "link": "http://arxiv.org/abs/2506.15065v1",
    "published": "2025-06-18T02:13:41+00:00",
    "summary_zh": "大型语言模型（LLMs）正日益成为具身智能体的认知核心。然而，源于无法将用户指令与观测物理环境锚定的继承性幻觉，可能导致导航错误——例如搜索根本不存在的冰箱。本文首次系统研究了在场景-任务不一致条件下执行长时程任务的LLM具身智能体所产生的幻觉现象。我们旨在厘清：幻觉发生的程度、触发幻觉的场景不一致类型，以及当前模型的应对方式。为此，我们在现有基准测试基础上构建了幻觉探测集，其诱发的幻觉率可达基础提示词的40倍。通过在两个仿真环境中评估12种模型，我们发现虽然模型具备推理能力，但无法解决场景-任务不一致问题——这凸显了处理不可行任务时的根本性局限。我们还针对每种场景提出了理想模型行为的可操作性见解，为开发更稳健可靠的规划策略提供了指导。"
  },
  {
    "title": "Truncated Proximal Policy Optimization",
    "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
    "link": "http://arxiv.org/abs/2506.15050v1",
    "published": "2025-06-18T01:21:38+00:00",
    "summary_zh": "近年来，测试时扩展的大语言模型（LLMs）通过生成长链式思维（CoT），在科学和专业任务中展现出卓越的推理能力。作为开发这类推理模型的关键组件，以近端策略优化（PPO）及其变体为代表的强化学习方法，使模型能够通过试错进行学习。然而，由于PPO固有的同策略特性，其训练过程耗时较长，且响应长度的增加会进一步加剧这一问题。\n\n本研究提出截断近端策略优化（T-PPO）——PPO的新型改进方案，通过简化策略更新流程和限制响应生成长度来提升训练效率。T-PPO有效缓解了完整生成流程中硬件利用率低下的固有缺陷（即在等待完整轨迹完成期间资源常处于闲置状态）。我们的贡献包含两方面：首先，提出扩展广义优势估计（EGAE），可在保持策略学习完整性的前提下，基于不完整响应进行优势估计；其次，设计计算优化机制实现策略模型与价值模型的独立优化。该机制通过选择性过滤提示词与截断标记，在保证收敛性能的同时减少冗余计算，显著加速训练过程。\n\n我们在320亿参数基模型上以AIME 2024数据集验证T-PPO的有效性。实验结果表明，T-PPO将推理LLMs的训练效率提升高达2.5倍，并超越现有同类方法。"
  },
  {
    "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers",
    "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.",
    "link": "http://arxiv.org/abs/2506.15047v1",
    "published": "2025-06-18T01:16:09+00:00",
    "summary_zh": "阿尔茨海默病及相关痴呆症（AD/ADRD）患者的家庭照护者面临着巨大的情感与事务性挑战，这使他们承受着更高的压力、焦虑和抑郁风险。尽管生成式人工智能（尤其是大语言模型）的最新进展为心理健康支持提供了新机遇，但关于照护者如何认知和运用这类技术的研究仍十分有限。为填补这一空白，我们开发了基于GPT-4o的聊天机器人Carey，旨在为AD/ADRD照护者提供信息支持和情感支持。以Carey作为技术探针，我们在常见照护压力情境下对16名家庭照护者进行了半结构化访谈。通过归纳编码和反思性主题分析，我们系统梳理了照护者在六个维度上的需求与期望——即时信息获取、情感支持、安全倾诉空间、危机管理、个性化服务及数据隐私。针对每个维度，我们还识别出照护者期望与顾虑之间的微妙张力。本研究绘制了照护者需求图谱、AI聊天机器人的优势与不足，并提出了设计建议。这些发现为构建主动响应、值得信赖且以照护者为中心的AI系统提供了理论与实践依据，有助于更好地满足AD/ADRD照护者不断演变的心理健康需求。"
  },
  {
    "title": "Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models",
    "summary": "As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.",
    "link": "http://arxiv.org/abs/2506.15041v1",
    "published": "2025-06-18T01:00:59+00:00",
    "summary_zh": "近年来，随着人们对经济叙事研究的兴趣日益增长，专门用于从文本中提取此类叙事的流程也不断增多。这些流程通常采用包括BERT等尖端自然语言处理技术的组合来完成该任务。虽然这类模型在叙事提取所需的基础语言处理操作（如语义角色标注）上表现有效，但它们缺乏区分\"提取经济叙事\"与\"执行经典语言任务\"所需的深层语义理解能力。我们未采用复杂的模型流程，而是通过分析《华尔街日报》和《纽约时报》关于通胀主题的新闻报道语料库，评估了大语言模型（LLMs）的应用价值。我们采用严格的叙事定义标准，将GPT-4o的输出结果与专家标注员生成的金标准叙事进行对比。研究结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时仍未能达到专家级水平。鉴于大语言模型在经济研究领域的创新性应用，我们还为未来社会科学领域运用LLMs开展类似研究提供了指导建议。"
  }
]